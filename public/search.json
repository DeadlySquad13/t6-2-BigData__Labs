[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Docs",
    "section": "",
    "text": "FROZEN_LAKE_ENV = { “name”: “FrozenLake-v1”, “observation_dim”: 16, # Массив действий в соответствии с документацией “actions_variants”: [0, 1, 2, 3], }"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#задание",
    "href": "index.html#задание",
    "title": "Docs",
    "section": "Задание",
    "text": "Задание\nНа основе рассмотренного на лекции примера реализуйте алгоритм Policy Iteration для любой среды обучения с подкреплением (кроме рассмотренной на лекции среды Toy Text / Frozen Lake) из библиотеки Gym (или аналогичной библиотеки)."
  },
  {
    "objectID": "index.html#выполнение",
    "href": "index.html#выполнение",
    "title": "Docs",
    "section": "Выполнение",
    "text": "Выполнение\nИсходный код программы:\nfrom pprint import pprint\n\nimport gym\nimport numpy as np\nfrom toy_environment.consts import CLIFF_WALKING_ENV\n\nENV = CLIFF_WALKING_ENV\n\n\nclass PolicyIterationAgent:\n    \"\"\"\n    Класс, эмулирующий работу агента\n    \"\"\"\n\n    def __init__(self, env):\n        self.env = env\n        # Пространство состояний\n        self.observation_dim = ENV[\"observation_dim\"]\n        self.actions_variants = np.array(ENV[\"actions_variants\"])\n        # Задание стратегии (политики)\n        # Карта 4х4 и 4 возможных действия\n        self.policy_probs = np.full(\n            (self.observation_dim, len(self.actions_variants)), 1 / len(ENV[\"actions_variants\"])\n        )\n        # Начальные значения для v(s)\n        self.state_values = np.zeros(shape=(self.observation_dim))\n        # Начальные значения параметров\n        self.maxNumberOfIterations = 100_000\n        self.theta = 1e-6\n        self.gamma = 0.99\n\n    def print_policy(self):\n        \"\"\"\n        Вывод матриц стратегии\n        \"\"\"\n        print(\"Стратегия:\")\n        pprint(self.policy_probs)\n\n    def policy_evaluation(self):\n        \"\"\"\n        Оценивание стратегии\n        \"\"\"\n        # Предыдущее значение функции ценности\n        valueFunctionVector = self.state_values\n        for iterations in range(self.maxNumberOfIterations):\n            # Новое значение функции ценности\n            valueFunctionVectorNextIteration = np.zeros(shape=(self.observation_dim))\n            # Цикл по состояниям\n            for state in range(self.observation_dim):\n                # Вероятности действий\n                action_probabilities = self.policy_probs[state]\n                # Цикл по действиям\n                outerSum = 0\n                for action, prob in enumerate(action_probabilities):\n                    innerSum = 0\n                    # Цикл по вероятностям действий\n                    for probability, next_state, reward, isTerminalState in self.env.P[\n                        state\n                    ][action]:\n                        innerSum = innerSum + probability * (\n                            reward + self.gamma * self.state_values[next_state]\n                        )\n                    outerSum = outerSum + self.policy_probs[state][action] * innerSum\n                valueFunctionVectorNextIteration[state] = outerSum\n            if (\n                np.max(np.abs(valueFunctionVectorNextIteration - valueFunctionVector))\n                &lt; self.theta\n            ):\n                # Проверка сходимости алгоритма\n                valueFunctionVector = valueFunctionVectorNextIteration\n                break\n            valueFunctionVector = valueFunctionVectorNextIteration\n        return valueFunctionVector\n\n    def policy_improvement(self):\n        \"\"\"\n        Улучшение стратегии\n        \"\"\"\n        qvaluesMatrix = np.zeros((self.observation_dim, len(self.actions_variants)))\n        improvedPolicy = np.zeros((self.observation_dim, len(self.actions_variants)))\n        # Цикл по состояниям\n        for state in range(self.observation_dim):\n            for action in range(len(self.actions_variants)):\n                for probability, next_state, reward, isTerminalState in self.env.P[\n                    state\n                ][action]:\n                    qvaluesMatrix[state, action] = qvaluesMatrix[\n                        state, action\n                    ] + probability * (\n                        reward + self.gamma * self.state_values[next_state]\n                    )\n\n            # Находим лучшие индексы\n            bestActionIndex = np.where(\n                qvaluesMatrix[state, :] == np.max(qvaluesMatrix[state, :])\n            )\n            # Обновление стратегии\n            improvedPolicy[state, bestActionIndex] = 1 / np.size(bestActionIndex)\n        return improvedPolicy\n\n    def policy_iteration(self, cnt):\n        \"\"\"\n        Основная реализация алгоритма\n        \"\"\"\n        policy_stable = False\n        for i in range(1, cnt + 1):\n            self.state_values = self.policy_evaluation()\n            self.policy_probs = self.policy_improvement()\n        print(f\"Алгоритм выполнился за {i} шагов.\")\n\n\ndef play_agent(agent):\n    env = gym.make(ENV[\"name\"], render_mode=\"human\")\n    state = env.reset()[0]\n    done = False\n    while not done:\n        p = agent.policy_probs[state]\n        if isinstance(p, np.ndarray):\n            action = np.random.choice(len(agent.actions_variants), p=p)\n        else:\n            action = p\n        next_state, reward, terminated, truncated, _ = env.step(action)\n        env.render()\n        state = next_state\n        if terminated or truncated:\n            done = True\n\n\ndef main():\n    # Создание среды\n    env = gym.make(ENV[\"name\"])\n    env.reset()\n\n    # Обучение агента\n    agent = PolicyIterationAgent(env)\n    agent.print_policy()\n    agent.policy_iteration(100_000)\n    agent.print_policy()\n\n    # Проигрывание сцены для обученного агента\n    play_agent(agent)\n\n\nif __name__ == \"__main__\":\n    main()\nИ содержимое toy_environment.consts:\n# https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\nFROZEN_LAKE_ENV = {\n    \"name\": \"FrozenLake-v1\",\n    \"observation_dim\": 16,\n    # Массив действий в соответствии с документацией\n    \"actions_variants\": [0, 1, 2, 3],\n}\n\n# https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\nCLIFF_WALKING_ENV = {\n    \"name\": \"CliffWalking-v0\",\n    \"observation_dim\": 48,\n    # Массив действий в соответствии с документацией\n    \"actions_variants\": [0, 1, 2, 3],\n}"
  },
  {
    "objectID": "index.html#prepare-environment-for-development",
    "href": "index.html#prepare-environment-for-development",
    "title": "Docs",
    "section": "Prepare environment for development",
    "text": "Prepare environment for development\n\nInstall pixi package manager\nFollow instructions from: https://prefix.dev/docs/pixi/overview#installation You can also install it on ArchLinux by using yay pixi. With nix pixi package is also available on unstable channel.\nActivate virtual environment: pixi shell.\n\nI also recommend installing direnv and running direnv allow .envrc: it will automagically activate environment once you open the directory of the Project and deactivate it once you leave it.\n\nInstall dependencies: pixi install.\n\nNow you’re ready to go! You have just installed all dependencies for project and development.\nNow all subfolders of src/ are installed as local dev packages for easier management.\nAs for development tooling you have now: - flake8 for linting (pixi run lint-check) - black for code formatting (pixi run format-check or to also fix if possible: pixi run format) - isort for sorting imports (pixi run order-imports-check or to fix if possible: pixi run order-imports) - mypy for static type checking (pixi run types-check) - pytest for testing (you can use pixi run test to run all tests) - pre-commit for running all these tools on commit (you can use pixi run pre-commit-check to check)"
  },
  {
    "objectID": "index.html#prepare-code-editor-for-development",
    "href": "index.html#prepare-code-editor-for-development",
    "title": "Docs",
    "section": "Prepare code editor for development",
    "text": "Prepare code editor for development\n\nNeovim\nInstall none-ls - it will hook up all installed tools in your environment. Run :NullLsInfo in neovim to check status.\n\n\nVSCode\nInstall related extension for each tool: - extension for flake8 - extension for black - extension for isort - extension for mypy\nDon’t forget to change required settings in your VSCode settings file. For example:\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"ms-python.black-formatter\",\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n        \"source.organizeImports\": \"explicit\"\n    },\n  },\n  \"isort.args\":[\"--profile\", \"black\"],\nDon’t fiddle with detailed settings of instruments: they’re already configured locally in project. Change only editor-related options."
  },
  {
    "objectID": "index.html#create-new-package",
    "href": "index.html#create-new-package",
    "title": "Docs",
    "section": "Create new package",
    "text": "Create new package\nTo create new package: 1. Create new directory in src/&lt;my_package_name&gt; 2. Add empty __init__.py and empty py.typed 3. Add your package  to ‘packages’ list in setup.cfg\nNow you can write code in your new package and use it in other modules by importing it via from &lt;my_package_name&gt;.&lt;file&gt; import &lt;function&gt;! You can also do import it that way in tests."
  },
  {
    "objectID": "index.html#testing",
    "href": "index.html#testing",
    "title": "Docs",
    "section": "Testing",
    "text": "Testing\nRun rixi run test - it will run pytest for all files in tests/ directory with options set in pytest.ini."
  },
  {
    "objectID": "index.html#running-in-docker",
    "href": "index.html#running-in-docker",
    "title": "Docs",
    "section": "Running in Docker",
    "text": "Running in Docker\nYou can also use Docker to run everything in containerized environment without installing anything.\nFirst of all, you have to set Docker related variables in .env, see .env.example - you can pick any name you wish. After that you can use make commands for common development workflow: 1. Build image: make build-image 2. Start container: make start 3. Run any of the checks (commands have similar names to pixi run ... commands): - make lint-check - make format-check - make order-imports-check - make types-check\n\nConnect to image if you want: make connect\nAnd finally, stop once finished working: make stop\n\n\nOn Windows you should install make or just use commands from Makefile manually"
  },
  {
    "objectID": "index.html#making-changes-to-cicd-pipeline",
    "href": "index.html#making-changes-to-cicd-pipeline",
    "title": "Docs",
    "section": "Making Changes to CI/CD Pipeline",
    "text": "Making Changes to CI/CD Pipeline\nUse kebab-style for naming stages and jobs as it’s common style in pixi tasks and make targets.\nfrom pprint import pprint\n\nimport gym\nimport numpy as np\nfrom toy_environment.consts import CLIFF_WALKING_ENV\n\nENV = CLIFF_WALKING_ENV\n\n\nclass PolicyIterationAgent:\n    \"\"\"\n    Класс, эмулирующий работу агента\n    \"\"\"\n\n    def __init__(self, env):\n        self.env = env\n        # Пространство состояний\n        self.observation_dim = ENV[\"observation_dim\"]\n        self.actions_variants = np.array(ENV[\"actions_variants\"])\n        # Задание стратегии (политики)\n        # Карта 4х4 и 4 возможных действия\n        self.policy_probs = np.full(\n            (self.observation_dim, len(self.actions_variants)), 1 / len(ENV[\"actions_variants\"])\n        )\n        # Начальные значения для v(s)\n        self.state_values = np.zeros(shape=(self.observation_dim))\n        # Начальные значения параметров\n        self.maxNumberOfIterations = 100_000\n        self.theta = 1e-6\n        self.gamma = 0.99\n\n    def print_policy(self):\n        \"\"\"\n        Вывод матриц стратегии\n        \"\"\"\n        print(\"Стратегия:\")\n        pprint(self.policy_probs)\n\n    def policy_evaluation(self):\n        \"\"\"\n        Оценивание стратегии\n        \"\"\"\n        # Предыдущее значение функции ценности\n        valueFunctionVector = self.state_values\n        for iterations in range(self.maxNumberOfIterations):\n            # Новое значение функции ценности\n            valueFunctionVectorNextIteration = np.zeros(shape=(self.observation_dim))\n            # Цикл по состояниям\n            for state in range(self.observation_dim):\n                # Вероятности действий\n                action_probabilities = self.policy_probs[state]\n                # Цикл по действиям\n                outerSum = 0\n                for action, prob in enumerate(action_probabilities):\n                    innerSum = 0\n                    # Цикл по вероятностям действий\n                    for probability, next_state, reward, isTerminalState in self.env.P[\n                        state\n                    ][action]:\n                        innerSum = innerSum + probability * (\n                            reward + self.gamma * self.state_values[next_state]\n                        )\n                    outerSum = outerSum + self.policy_probs[state][action] * innerSum\n                valueFunctionVectorNextIteration[state] = outerSum\n            if (\n                np.max(np.abs(valueFunctionVectorNextIteration - valueFunctionVector))\n                &lt; self.theta\n            ):\n                # Проверка сходимости алгоритма\n                valueFunctionVector = valueFunctionVectorNextIteration\n                break\n            valueFunctionVector = valueFunctionVectorNextIteration\n        return valueFunctionVector\n\n    def policy_improvement(self):\n        \"\"\"\n        Улучшение стратегии\n        \"\"\"\n        qvaluesMatrix = np.zeros((self.observation_dim, len(self.actions_variants)))\n        improvedPolicy = np.zeros((self.observation_dim, len(self.actions_variants)))\n        # Цикл по состояниям\n        for state in range(self.observation_dim):\n            for action in range(len(self.actions_variants)):\n                for probability, next_state, reward, isTerminalState in self.env.P[\n                    state\n                ][action]:\n                    qvaluesMatrix[state, action] = qvaluesMatrix[\n                        state, action\n                    ] + probability * (\n                        reward + self.gamma * self.state_values[next_state]\n                    )\n\n            # Находим лучшие индексы\n            bestActionIndex = np.where(\n                qvaluesMatrix[state, :] == np.max(qvaluesMatrix[state, :])\n            )\n            # Обновление стратегии\n            improvedPolicy[state, bestActionIndex] = 1 / np.size(bestActionIndex)\n        return improvedPolicy\n\n    def policy_iteration(self, cnt):\n        \"\"\"\n        Основная реализация алгоритма\n        \"\"\"\n        policy_stable = False\n        for i in range(1, cnt + 1):\n            self.state_values = self.policy_evaluation()\n            self.policy_probs = self.policy_improvement()\n        print(f\"Алгоритм выполнился за {i} шагов.\")\n\n\ndef play_agent(agent):\n    env = gym.make(ENV[\"name\"], render_mode=\"human\")\n    state = env.reset()[0]\n    done = False\n    while not done:\n        p = agent.policy_probs[state]\n        if isinstance(p, np.ndarray):\n            action = np.random.choice(len(agent.actions_variants), p=p)\n        else:\n            action = p\n        next_state, reward, terminated, truncated, _ = env.step(action)\n        env.render()\n        state = next_state\n        if terminated or truncated:\n            done = True\n\n\ndef main():\n    # Создание среды\n    env = gym.make(ENV[\"name\"])\n    env.reset()\n\n    # Обучение агента\n    agent = PolicyIterationAgent(env)\n    agent.print_policy()\n    agent.policy_iteration(100_000)\n    agent.print_policy()\n\n    # Проигрывание сцены для обученного агента\n    play_agent(agent)\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "index.html#prepare-environment-for-development-1",
    "href": "index.html#prepare-environment-for-development-1",
    "title": "Docs",
    "section": "Prepare environment for development",
    "text": "Prepare environment for development\n\nInstall pixi package manager\nFollow instructions from: https://prefix.dev/docs/pixi/overview#installation You can also install it on ArchLinux by using yay pixi. With nix pixi package is also available on unstable channel.\nActivate virtual environment: pixi shell.\n\nI also recommend installing direnv and running direnv allow .envrc: it will automagically activate environment once you open the directory of the Project and deactivate it once you leave it.\n\nInstall dependencies: pixi install.\n\nNow you’re ready to go! You have just installed all dependencies for project and development.\nNow all subfolders of src/ are installed as local dev packages for easier management.\nAs for development tooling you have now: - flake8 for linting (pixi run lint-check) - black for code formatting (pixi run format-check or to also fix if possible: pixi run format) - isort for sorting imports (pixi run order-imports-check or to fix if possible: pixi run order-imports) - mypy for static type checking (pixi run types-check) - pytest for testing (you can use pixi run test to run all tests) - pre-commit for running all these tools on commit (you can use pixi run pre-commit-check to check)"
  },
  {
    "objectID": "index.html#prepare-code-editor-for-development-1",
    "href": "index.html#prepare-code-editor-for-development-1",
    "title": "Docs",
    "section": "Prepare code editor for development",
    "text": "Prepare code editor for development\n\nNeovim\nInstall none-ls - it will hook up all installed tools in your environment. Run :NullLsInfo in neovim to check status.\n\n\nVSCode\nInstall related extension for each tool: - extension for flake8 - extension for black - extension for isort - extension for mypy\nDon’t forget to change required settings in your VSCode settings file. For example:\n  \"[python]\": {\n    \"editor.defaultFormatter\": \"ms-python.black-formatter\",\n    \"editor.formatOnSave\": true,\n    \"editor.codeActionsOnSave\": {\n        \"source.organizeImports\": \"explicit\"\n    },\n  },\n  \"isort.args\":[\"--profile\", \"black\"],\nDon’t fiddle with detailed settings of instruments: they’re already configured locally in project. Change only editor-related options."
  },
  {
    "objectID": "index.html#create-new-package-1",
    "href": "index.html#create-new-package-1",
    "title": "Docs",
    "section": "Create new package",
    "text": "Create new package\nTo create new package: 1. Create new directory in src/&lt;my_package_name&gt; 2. Add empty __init__.py and empty py.typed 3. Add your package  to ‘packages’ list in setup.cfg\nNow you can write code in your new package and use it in other modules by importing it via from &lt;my_package_name&gt;.&lt;file&gt; import &lt;function&gt;! You can also do import it that way in tests."
  },
  {
    "objectID": "index.html#testing-1",
    "href": "index.html#testing-1",
    "title": "Docs",
    "section": "Testing",
    "text": "Testing\nRun rixi run test - it will run pytest for all files in tests/ directory with options set in pytest.ini."
  },
  {
    "objectID": "index.html#running-in-docker-1",
    "href": "index.html#running-in-docker-1",
    "title": "Docs",
    "section": "Running in Docker",
    "text": "Running in Docker\nYou can also use Docker to run everything in containerized environment without installing anything.\nFirst of all, you have to set Docker related variables in .env, see .env.example - you can pick any name you wish. After that you can use make commands for common development workflow: 1. Build image: make build-image 2. Start container: make start 3. Run any of the checks (commands have similar names to pixi run ... commands): - make lint-check - make format-check - make order-imports-check - make types-check\n\nConnect to image if you want: make connect\nAnd finally, stop once finished working: make stop\n\n\nOn Windows you should install make or just use commands from Makefile manually"
  },
  {
    "objectID": "index.html#making-changes-to-cicd-pipeline-1",
    "href": "index.html#making-changes-to-cicd-pipeline-1",
    "title": "Docs",
    "section": "Making Changes to CI/CD Pipeline",
    "text": "Making Changes to CI/CD Pipeline\nUse kebab-style for naming stages and jobs as it’s common style in pixi tasks and make targets."
  },
  {
    "objectID": "reports/policy_iteration.html#задание",
    "href": "reports/policy_iteration.html#задание",
    "title": "Docs",
    "section": "Задание",
    "text": "Задание\nНа основе рассмотренного на лекции примера реализуйте алгоритм Policy Iteration для любой среды обучения с подкреплением (кроме рассмотренной на лекции среды Toy Text / Frozen Lake) из библиотеки Gym (или аналогичной библиотеки)."
  },
  {
    "objectID": "reports/policy_iteration.html#выполнение",
    "href": "reports/policy_iteration.html#выполнение",
    "title": "Docs",
    "section": "Выполнение",
    "text": "Выполнение\nTatata: Александр Исходный код программы:\nfrom pprint import pprint\n\nimport gym\nimport numpy as np\nfrom toy_environment.consts import CLIFF_WALKING_ENV\n\nENV = CLIFF_WALKING_ENV\n\n\nclass PolicyIterationAgent:\n    \"\"\"\n    Класс, эмулирующий работу агента\n    \"\"\"\n\n    def __init__(self, env):\n        self.env = env\n        # Пространство состояний\n        self.observation_dim = ENV[\"observation_dim\"]\n        self.actions_variants = np.array(ENV[\"actions_variants\"])\n        # Задание стратегии (политики)\n        # Карта 4х4 и 4 возможных действия\n        self.policy_probs = np.full(\n            (self.observation_dim, len(self.actions_variants)), 1 / len(ENV[\"actions_variants\"])\n        )\n        # Начальные значения для v(s)\n        self.state_values = np.zeros(shape=(self.observation_dim))\n        # Начальные значения параметров\n        self.maxNumberOfIterations = 100_000\n        self.theta = 1e-6\n        self.gamma = 0.99\n\n    def print_policy(self):\n        \"\"\"\n        Вывод матриц стратегии\n        \"\"\"\n        print(\"Стратегия:\")\n        pprint(self.policy_probs)\n\n    def policy_evaluation(self):\n        \"\"\"\n        Оценивание стратегии\n        \"\"\"\n        # Предыдущее значение функции ценности\n        valueFunctionVector = self.state_values\n        for iterations in range(self.maxNumberOfIterations):\n            # Новое значение функции ценности\n            valueFunctionVectorNextIteration = np.zeros(shape=(self.observation_dim))\n            # Цикл по состояниям\n            for state in range(self.observation_dim):\n                # Вероятности действий\n                action_probabilities = self.policy_probs[state]\n                # Цикл по действиям\n                outerSum = 0\n                for action, prob in enumerate(action_probabilities):\n                    innerSum = 0\n                    # Цикл по вероятностям действий\n                    for probability, next_state, reward, isTerminalState in self.env.P[\n                        state\n                    ][action]:\n                        innerSum = innerSum + probability * (\n                            reward + self.gamma * self.state_values[next_state]\n                        )\n                    outerSum = outerSum + self.policy_probs[state][action] * innerSum\n                valueFunctionVectorNextIteration[state] = outerSum\n            if (\n                np.max(np.abs(valueFunctionVectorNextIteration - valueFunctionVector))\n                &lt; self.theta\n            ):\n                # Проверка сходимости алгоритма\n                valueFunctionVector = valueFunctionVectorNextIteration\n                break\n            valueFunctionVector = valueFunctionVectorNextIteration\n        return valueFunctionVector\n\n    def policy_improvement(self):\n        \"\"\"\n        Улучшение стратегии\n        \"\"\"\n        qvaluesMatrix = np.zeros((self.observation_dim, len(self.actions_variants)))\n        improvedPolicy = np.zeros((self.observation_dim, len(self.actions_variants)))\n        # Цикл по состояниям\n        for state in range(self.observation_dim):\n            for action in range(len(self.actions_variants)):\n                for probability, next_state, reward, isTerminalState in self.env.P[\n                    state\n                ][action]:\n                    qvaluesMatrix[state, action] = qvaluesMatrix[\n                        state, action\n                    ] + probability * (\n                        reward + self.gamma * self.state_values[next_state]\n                    )\n\n            # Находим лучшие индексы\n            bestActionIndex = np.where(\n                qvaluesMatrix[state, :] == np.max(qvaluesMatrix[state, :])\n            )\n            # Обновление стратегии\n            improvedPolicy[state, bestActionIndex] = 1 / np.size(bestActionIndex)\n        return improvedPolicy\n\n    def policy_iteration(self, cnt):\n        \"\"\"\n        Основная реализация алгоритма\n        \"\"\"\n        policy_stable = False\n        for i in range(1, cnt + 1):\n            self.state_values = self.policy_evaluation()\n            self.policy_probs = self.policy_improvement()\n        print(f\"Алгоритм выполнился за {i} шагов.\")\n\n\ndef play_agent(agent):\n    env = gym.make(ENV[\"name\"], render_mode=\"human\")\n    state = env.reset()[0]\n    done = False\n    while not done:\n        p = agent.policy_probs[state]\n        if isinstance(p, np.ndarray):\n            action = np.random.choice(len(agent.actions_variants), p=p)\n        else:\n            action = p\n        next_state, reward, terminated, truncated, _ = env.step(action)\n        env.render()\n        state = next_state\n        if terminated or truncated:\n            done = True\n\n\ndef main():\n    # Создание среды\n    env = gym.make(ENV[\"name\"])\n    env.reset()\n\n    # Обучение агента\n    agent = PolicyIterationAgent(env)\n    agent.print_policy()\n    agent.policy_iteration(100_000)\n    agent.print_policy()\n\n    # Проигрывание сцены для обученного агента\n    play_agent(agent)\n\n\nif __name__ == \"__main__\":\n    main()\nИ содержимое toy_environment.consts:\n# https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\nFROZEN_LAKE_ENV = {\n    \"name\": \"FrozenLake-v1\",\n    \"observation_dim\": 16,\n    # Массив действий в соответствии с документацией\n    \"actions_variants\": [0, 1, 2, 3],\n}\n\n# https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\nCLIFF_WALKING_ENV = {\n    \"name\": \"CliffWalking-v0\",\n    \"observation_dim\": 48,\n    # Массив действий в соответствии с документацией\n    \"actions_variants\": [0, 1, 2, 3],\n}"
  }
]