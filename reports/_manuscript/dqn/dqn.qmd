---
work:
    type: 'Лабораторная работа'
    theme: 'Обучение с подкреплением. Обучение на основе глубоких Q-сетей'
    number: '3'
---

{{< include ../title-page.qmd >}}
## Задание
На основе рассмотренных на лекции примеров реализуйте алгоритм DQN.
В качестве среды можно использовать классические среды (в этом случае используется полносвязная архитектура нейронной сети).

## Выполнение
Исходный код программы:
```python
{{< include ../../src/dqn/main.py >}}
```

### Результат
Протестируем работу алгоритма в разных [классических средах](https://www.gymlibrary.dev/environments/classic_control/).

#### CartPole
Проверим на среде [CartPole](https://www.gymlibrary.dev/environments/classic_control/cart_pole/).
Тележку можно либо двигать влево, либо вправо - необходимо как можно дольше продержать бревно
на ней в равновесии.

![CartPole](../execution_results/dqn/cartPole/demostration.png)

Попробуем запустить алгоритм с исходными параметрами.
![](../execution_results/dqn/cartPole/cuda.png)

Попробуем оптимизировать алгоритм, уменьшим количество слоёв до 2-х.
![](../execution_results/dqn/cartPole/cuda_2layers.png)

Как видно, качество значительно ухудшилось.

Попытаемся подобрать количество
нейронов на скрытых слоях. Уменьшим количество до 64.
![](../execution_results/dqn/cartPole/cuda_64.png)

Уменьшим количество до 96.
![](../execution_results/dqn/cartPole/cuda_96.png)

Как видно 3 слоя с 128 параметрами оказалось оптимальной комбинацией.

#### Acrobot
Проверим работу алгоритма на среде [Acrobot](https://www.gymlibrary.dev/environments/classic_control/acrobot/).
Можно толкать сустав либо влево, либо вправо. Можно оставить его впокое.

Необходимо выполнить обратную CartPole задачу: за **наименьшее** количество времени поднять его выше линии.
![Acrobot](../execution_results/dqn/acrobot/demostration.png)

Попытаемся подобрать количество нейронов в скрытом слое.

Попробуем 64:
![](../execution_results/dqn/acrobot/cuda_64.png)

Попробуем 96:
![](../execution_results/dqn/acrobot/cuda_96.png)

## Вывод
Алгоритм на основе глубоких сетей прекрасно справился с разными задачами в разных средах.
Изменение гиперпараметров не улучшило результат - нам повезло угадать их с самого начала:
3 слоя и 128 нейронов оптимальные. Увеличение слоёв или нейронов приведёт к усложнению модели
и увеличению времени обучения. Уменьшение слоёв или нейронов - портит качество модели.
