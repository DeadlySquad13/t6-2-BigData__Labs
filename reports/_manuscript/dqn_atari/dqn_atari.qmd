---
work:
    type: 'Домашняя работа'
    theme: 'Обучение с подкреплением. Обучение на основе глубоких Q-сетей в средах Atari'
    number: '1'
---

{{< include ../title-page.qmd >}}
## Задание
На основе рассмотренных на лекции примеров реализуйте алгоритм DQN с использованием свёрточных слоёв.
В качестве среды использовать игры Atari. 

## Выполнение
Исходный код программы:
```python
{{< include ../../src/dqn-atari/main.py >}}
```

Код модели:
```python
{{< include ../../src/dqn-atari/model.py >}}
```

Реализация wrapper для окружения с целью ускорения работы алгоритма [согласно статье][1]:
```python
{{< include ../../src/dqn-atari/wrapper.py >}}
```

Вспомогательные функции:
```python
{{< include ../../src/dqn-atari/utils.py >}}
```

## Детали реализации
Для ускорения обучения (на данный момент полное обучение занимает около
2 часов) были предприняты следующие решения:
- конвертировать RGB в Gray Scale (цвета нам не важны, ведь нас интересует только позиция элементов на экране)
- уменьшить картинку до 84*84
- стакать 4 картинки вместе для лучшего отслеживания причины-следствия.
- обрезать элементы интерфейса, не относящиеся непосредственно к игровому полю
- нормализовать картинку с [0, 255] до [0, 1].

## Результаты
### Breakout
Проверим работу алгоритма на игре [breakout](https://www.gymlibrary.dev/environments/atari/breakout/):

![](../execution_results/dqn_atari/breakout/dqn/9500.mp4){width=80%}

**Оптимальные параметры:**

- `epoch` = 10.001
- `eval-cycle` = 500

### Pong
Проверим работу на игре [pong](https://www.gymlibrary.dev/environments/atari/pong/).

К сожалению, в данной игре мы столкнулись с принципиально новыми сложностями. DQN не сходился вне
зависимости от параметров.

![](../execution_results/dqn_atari/pong/double_dueldqn/500.mp4){width=80%}

После ознакомления со [статьями][1] по реализации алгоритма
для данной игры, было выяснено, что из-за особенностей среды её нужно
обучать на DoubleDQN.

Добавив в код DoubleDQN, среду наконец-то получилось успешно обучить.

**Оптимальные параметры:**

- `epoch` = 501
- `eval-cycle` = 50

### Boxing
Проверим работу на игре [boxing](https://www.gymlibrary.dev/environments/atari/boxing/).

К несчастью, успешно обучить модель для данной игры не получилось: мы снова столкнулись с тем, что
данная среда не всегда сходится с DQN. На DoubleDQN она очень долго обучалась, поэтому были
предприняты попытки модифицировать DuelingDQN и DoubleDuelingDQN.

Игру ввиду временных ограничений так и не получилось обучить, но
новые алгоритмы получилось опробовать на Breakout:

![](../execution_results/dqn_atari/breakout/cmpbreakout.png){width=80%}

Как видно, DQN для данной игры показывает самый хороший результат.

## Вывод
Только double dqn сходится в Pong. Вероятнее всего это связано с тем, что
большинство переходов в Pong имеют награду 0, поэтому для агента тяжело
получить какие-то осмыленные переходы.

Модель намного лучше себя показывает в Breakout.

В целом, обучение с подкреплением даёт возможность решать задачи удивительными методами
без каких-либо глубоких знаний о среде. Тем не менее, от разработчика требуется большая интуиция
и понимание принципов среды для составления адекватно работающей модели.
В совокупности с длительным временем обучения (например, 2 часа для pong после всех оптимизаций скорости
обучения) все эти сложности приводят к тяжёлому процессу нахождения наилучшего решения.

[1]: <https://github.com/iewug/Atari-DQN/tree/master> 'PyTorch implementation of DQN for Atari'

